<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/README.md" />
              <option name="originalContent" value="**.env Contents**&#10;&#10;OPENAI_API_KEY=&#10;&#10;LANGCHAIN_API_KEY=&#10;&#10;LANGCHAIN_TRACING_V2=&#10;&#10;LANGCHAIN_PROJECT=&#10;&#10;TAVILY_API_KEY=&#10;&#10;PYTHONPATH=&#10;" />
              <option name="updatedContent" value="# Advanced RAG — Dashboard and Workflow&#10;&#10;A local Retrieval-Augmented Generation (RAG) pipeline with a Streamlit dashboard for interacting with the compiled graph workflow, ingesting documents, and visualizing the workflow graph.&#10;&#10;This repository contains a LangGraph-based workflow that performs retrieval, grading, generation and optional web search, plus a lightweight web UI for exploring and interacting with the system.&#10;&#10;## Features&#10;&#10;- Compiled graph workflow (graph/graph.py) implementing routing, retrieval, grading and generation.&#10;- Streamlit dashboard (`app.py`) with:&#10;  - Query System: submit questions to the compiled workflow&#10;  - Document Ingestion: upload local files (txt / pdf / docx) and index them into Chroma&#10;  - System Info: render and view the workflow diagram (graph.png)&#10;- Ingestion helper (`ingestion.py`) that:&#10;  - Seeds a Chroma collection from a set of web articles&#10;  - Provides `process_documents(file_paths: list[str])` to index local files&#10;&#10;## Requirements&#10;&#10;- Python 3.10+ (this project used Python 3.12)&#10;- A Python virtual environment is recommended&#10;- System dependency for Graphviz if you wish to render PNGs from the workflow graph (e.g. `brew install graphviz` on macOS)&#10;&#10;## Python dependencies&#10;&#10;All Python dependencies are listed in `requirements.txt`. Install with:&#10;&#10;```bash&#10;pip install -r requirements.txt&#10;```&#10;&#10;(If you add or update dependencies, re-run the above command in your virtualenv.)&#10;&#10;## Configuration&#10;&#10;The RAG system requires API credentials for the LLM and embeddings provider. By default it expects an OpenAI API key in the environment.&#10;&#10;- Set the environment variable `OPENAI_API_KEY` (or `OPENAI_KEY`) before running the app, or place it in a `.env` file at the project root.&#10;&#10;Example (macOS / Linux):&#10;&#10;```bash&#10;export OPENAI_API_KEY=&quot;sk-...&quot;&#10;```&#10;&#10;## Running the dashboard&#10;&#10;1. Activate your virtualenv where `requirements.txt` has been installed.&#10;2. Start the Streamlit dashboard:&#10;&#10;   ```bash&#10;   streamlit run app.py&#10;   ```&#10;&#10;3. Open the URL Streamlit prints (usually http://localhost:8501).&#10;&#10;Dashboard pages:&#10;- Query System — type a question and click Submit. The dashboard will lazy-import the compiled `app` from `main.py` and call `app.invoke(input={&quot;question&quot;: ...})`.&#10;- Document Ingestion — upload txt, pdf, doc, docx files and click `Process Documents` to index them into the Chroma collection used by the workflow.&#10;- System Info — click `Render Workflow Diagram` to try to create `graph.png` from the compiled graph, then view it in the page.&#10;&#10;## Ingestion notes&#10;&#10;- The repository seeds the Chroma collection from a small set of web pages defined in `ingestion.py`.&#10;- The `process_documents` helper (in `ingestion.py`) supports local `txt`, `pdf`, and `docx` files and will split and add chunks to the Chroma collection.&#10;- PDF extraction uses `pypdf`. DOCX extraction uses `python-docx`.&#10;- The Chroma collection persists to `./.chroma`.&#10;&#10;## Workflow graph&#10;&#10;- The graph is defined and compiled inside `graph/graph.py` as a `StateGraph`. The compiled object is exposed as `app`.&#10;- Graph rendering is optional and may require system Graphviz binaries. If `graph.draw_mermaid_png` is available it will be used to write `graph.png`.&#10;&#10;## Troubleshooting&#10;&#10;- Missing OpenAI key: the Streamlit UI will warn if `OPENAI_API_KEY` is not found. LLM/embeddings calls will fail without a valid key.&#10;- Graph rendering fails: install Graphviz system package (not just the Python binding) and retry (macOS: `brew install graphviz`).&#10;- PDF/DOCX parsing issues: ensure `pypdf` and `python-docx` are installed (they are listed in `requirements.txt`). Some PDFs may not produce perfect text extraction.&#10;- If ingestion does not appear to update the retriever results, confirm that the Chroma collection was persisted to `./.chroma` and the app is using the same persist directory.&#10;&#10;## Project layout (high level)&#10;&#10;- app.py — Streamlit dashboard and UI&#10;- ingestion.py — seeding and `process_documents` helper for indexing local files&#10;- main.py — small entrypoint that loads the compiled graph (`app`) from `graph/graph.py`&#10;- graph/ — LangGraph workflow and node/chain implementations&#10;  - graph/graph.py — builds and compiles the workflow&#10;  - graph/nodes/ — node implementations (retrieve, generate, grade...)&#10;  - graph/chains/ — chain implementations used by nodes&#10;- graph.png — optional workflow image&#10;&#10;## Development notes&#10;&#10;- The Streamlit UI lazy-imports the compiled `app` to avoid heavy startup at import time. When developing the graph you can run `python -m graph.graph` or open `main.py` to exercise the workflow programmatically.&#10;- When changing dependencies, update `requirements.txt` and reinstall.&#10;&#10;## Next steps / improvements&#10;&#10;- Show retrieved documents and source metadata alongside the generated answer in the dashboard.&#10;- Add a query history panel and per-step state visualization while the workflow runs.&#10;- Add authentication and remote deployment support (FastAPI + React or Streamlit Cloud).&#10;&#10;## License &amp; Contributing&#10;&#10;Feel free to open issues or submit PRs. No license file included — add one if you plan to open-source this project publicly.&#10;&#10;---&#10;&#10;If you want, I can also:&#10;- Add a richer response view in the dashboard (show retrieved chunks + sources).&#10;- Add a query history and export tool.&#10;&#10;Tell me which enhancement to implement next.&#10;&#10;**.env Contents**&#10;&#10;OPENAI_API_KEY=&#10;&#10;LANGCHAIN_API_KEY=&#10;&#10;LANGCHAIN_TRACING_V2=&#10;&#10;LANGCHAIN_PROJECT=&#10;&#10;TAVILY_API_KEY=&#10;&#10;PYTHONPATH=" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/ingestion.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/ingestion.py" />
              <option name="originalContent" value="from dotenv import load_dotenv&#10;from langchain.text_splitter import RecursiveCharacterTextSplitter&#10;from langchain_community.document_loaders import WebBaseLoader&#10;from langchain_community.vectorstores import Chroma&#10;from langchain_openai import OpenAIEmbeddings&#10;&#10;load_dotenv()&#10;&#10;urls = [&#10;    &quot;https://lilianweng.github.io/posts/2023-06-23-agent/&quot;,&#10;    &quot;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&quot;,&#10;    &quot;https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/&quot;,&#10;]&#10;&#10;docs = [WebBaseLoader(url).load() for url in urls]&#10;docs_list = [item for sublist in docs for item in sublist]&#10;&#10;text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(&#10;    chunk_size=250, chunk_overlap=0&#10;)&#10;doc_splits = text_splitter.split_documents(docs_list)&#10;&#10;vectorstore = Chroma.from_documents(&#10;     documents=doc_splits,&#10;     collection_name=&quot;rag-chroma&quot;,&#10;     embedding=OpenAIEmbeddings(),&#10;     persist_directory=&quot;./.chroma&quot;,&#10; )&#10;&#10;retriever = Chroma(&#10;    collection_name=&quot;rag-chroma&quot;,&#10;    persist_directory=&quot;./.chroma&quot;,&#10;    embedding_function=OpenAIEmbeddings(),&#10;).as_retriever()&#10;&#10;&#10;# New helper to process local files uploaded via the dashboard&#10;def process_documents(file_paths: list[str]):&#10;    &quot;&quot;&quot;&#10;    Load local files (txt, pdf, docx), split them into chunks and add to the existing Chroma collection.&#10;    &quot;&quot;&quot;&#10;    from langchain.schema import Document as LangDocument&#10;&#10;    loaded_docs = []&#10;    for path in file_paths:&#10;        try:&#10;            if path.lower().endswith('.txt'):&#10;                with open(path, 'r', encoding='utf-8', errors='ignore') as f:&#10;                    text = f.read()&#10;            elif path.lower().endswith('.pdf'):&#10;                # pypdf is used to extract text from PDFs&#10;                from pypdf import PdfReader&#10;&#10;                reader = PdfReader(path)&#10;                pages = [p.extract_text() or &quot;&quot; for p in reader.pages]&#10;                text = &quot;\n&quot;.join(pages)&#10;            elif path.lower().endswith('.docx') or path.lower().endswith('.doc'):&#10;                # python-docx for docx files&#10;                from docx import Document as DocxDocument&#10;&#10;                doc = DocxDocument(path)&#10;                paras = [p.text for p in doc.paragraphs]&#10;                text = &quot;\n&quot;.join(paras)&#10;            else:&#10;                # Fallback try to read as text&#10;                with open(path, 'rb') as f:&#10;                    try:&#10;                        text = f.read().decode('utf-8', errors='ignore')&#10;                    except Exception:&#10;                        text = &quot;&quot;&#10;&#10;            if text:&#10;                loaded_docs.append(LangDocument(page_content=text, metadata={&quot;source&quot;: path}))&#10;        except Exception as e:&#10;            # ignore problematic files but continue&#10;            print(f&quot;Failed to load {path}: {e}&quot;)&#10;&#10;    if not loaded_docs:&#10;        print(&quot;No documents loaded to index.&quot;)&#10;        return&#10;&#10;    # split and add&#10;    splits = text_splitter.split_documents(loaded_docs)&#10;    try:&#10;        vectorstore.add_documents(splits)&#10;        # persist to disk if supported&#10;        if hasattr(vectorstore, 'persist'):&#10;            try:&#10;                vectorstore.persist()&#10;            except Exception:&#10;                pass&#10;        print(f&quot;Indexed {len(splits)} document chunks into Chroma.&quot;)&#10;    except Exception as e:&#10;        print(f&quot;Failed to add documents to vectorstore: {e}&quot;)" />
              <option name="updatedContent" value="from dotenv import load_dotenv&#10;from langchain.text_splitter import RecursiveCharacterTextSplitter&#10;from langchain_community.document_loaders import WebBaseLoader&#10;from langchain_community.vectorstores import Chroma&#10;from langchain_openai import OpenAIEmbeddings&#10;&#10;load_dotenv()&#10;&#10;urls = [&#10;    &quot;https://lilianweng.github.io/posts/2023-06-23-agent/&quot;,&#10;    &quot;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&quot;,&#10;    &quot;https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/&quot;,&#10;]&#10;&#10;docs = [WebBaseLoader(url).load() for url in urls]&#10;docs_list = [item for sublist in docs for item in sublist]&#10;&#10;text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(&#10;    chunk_size=250, chunk_overlap=0&#10;)&#10;doc_splits = text_splitter.split_documents(docs_list)&#10;&#10;vectorstore = Chroma.from_documents(&#10;     documents=doc_splits,&#10;     collection_name=&quot;rag-chroma&quot;,&#10;     embedding=OpenAIEmbeddings(),&#10;     persist_directory=&quot;./.chroma&quot;,&#10; )&#10;&#10;retriever = Chroma(&#10;    collection_name=&quot;rag-chroma&quot;,&#10;    persist_directory=&quot;./.chroma&quot;,&#10;    embedding_function=OpenAIEmbeddings(),&#10;).as_retriever()&#10;&#10;&#10;# New helper to process local files uploaded via the dashboard&#10;def process_documents(file_paths: list[str]):&#10;    &quot;&quot;&quot;&#10;    Load local files (txt, pdf, docx), split them into chunks and add to the existing Chroma collection.&#10;    &quot;&quot;&quot;&#10;    from langchain.schema import Document as LangDocument&#10;&#10;    loaded_docs = []&#10;    for path in file_paths:&#10;        try:&#10;            if path.lower().endswith('.txt'):&#10;                with open(path, 'r', encoding='utf-8', errors='ignore') as f:&#10;                    text = f.read()&#10;            elif path.lower().endswith('.pdf'):&#10;                # pypdf is used to extract text from PDFs&#10;                from pypdf import PdfReader&#10;&#10;                reader = PdfReader(path)&#10;                pages = [p.extract_text() or &quot;&quot; for p in reader.pages]&#10;                text = &quot;\n&quot;.join(pages)&#10;            elif path.lower().endswith('.docx') or path.lower().endswith('.doc'):&#10;                # python-docx for docx files&#10;                from docx import Document as DocxDocument&#10;&#10;                doc = DocxDocument(path)&#10;                paras = [p.text for p in doc.paragraphs]&#10;                text = &quot;\n&quot;.join(paras)&#10;            else:&#10;                # Fallback try to read as text&#10;                with open(path, 'rb') as f:&#10;                    try:&#10;                        text = f.read().decode('utf-8', errors='ignore')&#10;                    except Exception:&#10;                        text = &quot;&quot;&#10;&#10;            if text:&#10;                loaded_docs.append(LangDocument(page_content=text, metadata={&quot;source&quot;: path}))&#10;        except Exception as e:&#10;            # ignore problematic files but continue&#10;            print(f&quot;Failed to load {path}: {e}&quot;)&#10;&#10;    if not loaded_docs:&#10;        print(&quot;No documents loaded to index.&quot;)&#10;        return&#10;&#10;    # split and add&#10;    splits = text_splitter.split_documents(loaded_docs)&#10;    try:&#10;        vectorstore.add_documents(splits)&#10;        # persist to disk if supported&#10;        if hasattr(vectorstore, 'persist'):&#10;            try:&#10;                vectorstore.persist()&#10;            except Exception:&#10;                pass&#10;        print(f&quot;Indexed {len(splits)} document chunks into Chroma.&quot;)&#10;    except Exception as e:&#10;        print(f&quot;Failed to add documents to vectorstore: {e}&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/requirements.txt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/requirements.txt" />
              <option name="originalContent" value="beautifulsoup4==4.12.3&#10;langchain==0.2.7&#10;langgraph==0.1.8&#10;langchainhub==0.1.20&#10;langchain-community==0.2.7&#10;tavily-python==0.3.4&#10;langchain-chroma==0.1.2&#10;python-dotenv==1.0.1&#10;pytest==8.2.2&#10;langchain-openai==0.1.16&#10;streamlit==1.32.0&#10;graphviz==0.20.1&#10;pypdf==3.13.0&#10;python-docx==0.8.11&#10;" />
              <option name="updatedContent" value="beautifulsoup4==4.12.3&#10;langchain==0.2.7&#10;langgraph==0.1.8&#10;langchainhub==0.1.20&#10;langchain-community==0.2.7&#10;tavily-python==0.3.4&#10;langchain-chroma==0.1.2&#10;python-dotenv==1.0.1&#10;pytest==8.2.2&#10;langchain-openai==0.1.16&#10;streamlit==1.32.0&#10;graphviz==0.20.1&#10;pypdf==3.13.0&#10;python-docx==0.8.11" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/tests/test_graph.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/tests/test_graph.py" />
              <option name="updatedContent" value="import types&#10;import pytest&#10;&#10;from graph import graph as rg&#10;from graph.node_constants import WEBSEARCH, RETRIEVE, GENERATE&#10;&#10;&#10;def test_route_question_routes_to_websearch(monkeypatch):&#10;    # Patch question_router.invoke to return an object with datasource == WEBSEARCH&#10;    monkeypatch.setattr(rg, &quot;question_router&quot;, types.SimpleNamespace(invoke=lambda input: types.SimpleNamespace(datasource=WEBSEARCH)))&#10;    state = {}&#10;    result = rg.route_question(state)&#10;    assert result == WEBSEARCH&#10;&#10;&#10;def test_route_question_routes_to_retrieve(monkeypatch):&#10;    # Patch question_router.invoke to return an object with datasource == 'vectorstore'&#10;    monkeypatch.setattr(rg, &quot;question_router&quot;, types.SimpleNamespace(invoke=lambda input: types.SimpleNamespace(datasource=&quot;vectorstore&quot;)))&#10;    state = {}&#10;    result = rg.route_question(state)&#10;    assert result == RETRIEVE&#10;&#10;&#10;def test_decide_to_generate_prefers_websearch_when_flag_set():&#10;    state = {&quot;web_search&quot;: True}&#10;    assert rg.decide_to_generate(state) == WEBSEARCH&#10;&#10;&#10;def test_decide_to_generate_generates_when_no_web_search():&#10;    state = {&quot;web_search&quot;: False}&#10;    assert rg.decide_to_generate(state) == GENERATE&#10;&#10;&#10;def test_grade_generation_hallucination_false(monkeypatch):&#10;    # If hallucination_grader.invoke returns object with binary_score falsy -&gt; 'not supported'&#10;    monkeypatch.setattr(rg, &quot;hallucination_grader&quot;, types.SimpleNamespace(invoke=lambda input: types.SimpleNamespace(binary_score=False)))&#10;    state = {&quot;question&quot;: &quot;q&quot;, &quot;documents&quot;: [], &quot;generation&quot;: &quot;g&quot;}&#10;    assert rg.grade_generation_grounded_in_documents_and_question(state) == &quot;not supported&quot;&#10;&#10;&#10;def test_grade_generation_grounded_and_answer_useful(monkeypatch):&#10;    # hallucination true, answer_grader returns binary_score True -&gt; 'useful'&#10;    monkeypatch.setattr(rg, &quot;hallucination_grader&quot;, types.SimpleNamespace(invoke=lambda input: types.SimpleNamespace(binary_score=True)))&#10;    monkeypatch.setattr(rg, &quot;answer_grader&quot;, types.SimpleNamespace(invoke=lambda input: types.SimpleNamespace(binary_score=True)))&#10;    state = {&quot;question&quot;: &quot;q&quot;, &quot;documents&quot;: [&quot;d&quot;], &quot;generation&quot;: &quot;g&quot;}&#10;    assert rg.grade_generation_grounded_in_documents_and_question(state) == &quot;useful&quot;&#10;&#10;&#10;def test_grade_generation_grounded_but_not_answering(monkeypatch):&#10;    # hallucination true, answer_grader returns binary_score False -&gt; 'not useful'&#10;    monkeypatch.setattr(rg, &quot;hallucination_grader&quot;, types.SimpleNamespace(invoke=lambda input: types.SimpleNamespace(binary_score=True)))&#10;    monkeypatch.setattr(rg, &quot;answer_grader&quot;, types.SimpleNamespace(invoke=lambda input: types.SimpleNamespace(binary_score=False)))&#10;    state = {&quot;question&quot;: &quot;q&quot;, &quot;documents&quot;: [&quot;d&quot;], &quot;generation&quot;: &quot;g&quot;}&#10;    assert rg.grade_generation_grounded_in_documents_and_question(state) == &quot;not useful&quot;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/tests/test_project_files.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/tests/test_project_files.py" />
              <option name="updatedContent" value="import os&#10;import importlib&#10;&#10;&#10;def test_readme_exists():&#10;    assert os.path.isfile(&quot;README.md&quot;), &quot;README.md must exist at project root&quot;&#10;&#10;&#10;def test_requirements_contains_streamlit():&#10;    assert os.path.isfile(&quot;requirements.txt&quot;), &quot;requirements.txt must exist&quot;&#10;    with open(&quot;requirements.txt&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:&#10;        req = f.read().lower()&#10;    assert &quot;streamlit&quot; in req, &quot;requirements.txt should include streamlit&quot;&#10;&#10;&#10;def test_graph_module_has_expected_functions():&#10;    # Import the compiled graph module and check key functions are present&#10;    mod = importlib.import_module(&quot;graph.graph&quot;)&#10;    for name in (&quot;route_question&quot;, &quot;decide_to_generate&quot;, &quot;grade_generation_grounded_in_documents_and_question&quot;):&#10;        assert hasattr(mod, name), f&quot;graph.graph should define {name}&quot;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>